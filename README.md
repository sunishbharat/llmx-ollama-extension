# LLMX with Ollama extension 
This repository is a fork of llmx with added support for running Ollama models locally.
It extends llmx by integrating locally hosted Ollama models and their execution features.
You can install this fork directly from the GitHub repository using pip.

Use this version if you want seamless integration of Ollama models within the llmx workflow.
Contributions and feedback are welcome to further improve Ollama compatibility.




## Citation

If you use this library in your work, please cite:

```bibtex
@software{victordibiallmx,
author = {Victor Dibia},
license = {MIT},
month =  {10},
title = {LLMX - An API for Chat Fine-Tuned Language Models},
url = {https://github.com/victordibia/llmx},
year = {2023}
}
```
